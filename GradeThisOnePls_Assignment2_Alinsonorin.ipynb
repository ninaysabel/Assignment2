{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling and Exploratory Data Analysis\n",
        "## Do Q1 and Q2, and one other question."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/ninaysabel/Assignment2.git"
      ],
      "metadata": {
        "id": "5ud4uu2hMeQg",
        "outputId": "094f0be8-44cf-4eb9-fa78-85c518dab878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5ud4uu2hMeQg",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Assignment2'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 40 (delta 9), reused 5 (delta 5), pack-reused 27\u001b[K\n",
            "Receiving objects: 100% (40/40), 5.69 MiB | 5.86 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1 Responses**\n",
        "1. This paper is about tidying data- specifically, cleaning multiple data sets in a manner which is organizational and efficient.\n",
        "2. The \"tidy data standard\" allows for variables in a data set to be explored and analyzed by seeing the relationships drawn from them.\n",
        "3.\n",
        "  - There are many ways in which datasets can be \"messy\"- there is no one uniform way (certain rows having NaN values, names of columns being different but having the same values, etc.).\n",
        "  - Typically, variables' correlations lead to an observation in a cause-and-effect manner, but it can get confusing trying to discern what exactly (variables) is causing a certain outcome (observation).\n",
        "4.\n",
        "  - values: an item which is able to be quantitatively or qualitatively measured.\n",
        "  - variables: a collective item which measures something with a similarity despite a difference in units.\n",
        "  - observations: a collective item which measures something with a similar unit despite a difference in general similarity.\n",
        "\n",
        "5. \"Tidy data\" contains a variable and observation through a column and row, respectively, which goes on to form a table.\n",
        "6.\n",
        "  - 5 most common problems with datasets include having: variables stored in both rows and columns, tables including a single observational unit, values as column headers, different observational units saved in a singular table, and a column measuring multiple variables.\n",
        "  - The data in Table 4 is messy because it measures religion, income, and frequency across multiple columns and rows, when there should only be 3 rows per the respective variables.\n",
        "  - \"Melting\" a dataset involves turning columns into rows.\n",
        "7. Table 11 is messy as it has multiple columns measuring the same thing (ex: having a \"year\" and \"month\" column, and multiple value columns). Table 12 is not messy as it has combined the columns which share a value (ex: \"date\" and \"value\").\n",
        "8. The \"chicken-and-egg\" problem involves the intersectionality between tidy tools and tidy data; where, if we get co-dependent on one for the other, it creates a useless codependency. Wickham hopes to expand and find new ways in tackling data wrangling and storage."
      ],
      "metadata": {
        "id": "e9sBhExBOUQ-"
      },
      "id": "e9sBhExBOUQ-"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_NYC.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#1 Numeric Variable\n",
        "nyc_data = pd.read_csv('./Assignment2/data/airbnb_hw.csv')\n",
        "prices = nyc_data[\"Price\"]\n",
        "prices = prices.replace(',', '', regex=True) #get rid of commas (ex: $1,000 is now 1000)\n",
        "print(prices.isnull().sum()) #see how many NaN/missing values left (returns 0)\n",
        "\n",
        "#2 Categorical Variable\n",
        "sharky = pd.read_csv('./Assignment2/data/sharks.csv')\n",
        "sharky = sharky['Type'].replace({\"Boatomg\": \"Boating\", \"Boat\": \"Boating\", \"Questionable\": \"Unverified\", \"Unconfirmed\": \"Unverified\", \"Under investigation\": \"Unverified\"})\n",
        "#combine mispellings of shark type \"boat\" into one value, combine all shark type \"unknown\" as \"unverified\" but keeping shark type \"invalid\" and \"unverified\" separate in case they mean two separate things\n",
        "sharky.value_counts() #see how many of each shark type there are after combining spelling/redundant errors\n",
        "\n",
        "#3 Dummy Variable\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "pt = pd.read_csv('./drive/MyDrive/DS3001/VirginiaPretrialData2017.csv')\n",
        "var = 'WhetherDefendantWasReleasedPretrial'\n",
        "pt[var+'_nan'] = pt[var].isnull()\n",
        "pt[var].fillna(value=np.nan)\n",
        "\n",
        "#4 Missing Values\n",
        "pt = pd.read_csv('./drive/MyDrive/DS3001/VirginiaPretrialData2017.csv')\n",
        "\n",
        "pt['ImposedSentenceAllChargeInContactEvent']= pt['ImposedSentenceAllChargeInContactEvent'].map(str)\n",
        "pt['SentenceTypeAllChargesAtConvictionInContactEvent']= pt['SentenceTypeAllChargesAtConvictionInContactEvent'].map(str) #convert both variables' values as strings so you can concatenate them\n",
        "\n",
        "pt['ImposedSentenceAllChargeInContactEvent'].where(pt['ImposedSentenceAllChargeInContactEvent'].notnull(), None)\n",
        "pt['SentenceTypeAllChargesAtConvictionInContactEvent'].where(pt['SentenceTypeAllChargesAtConvictionInContactEvent'].notnull(), None) #fill in missing values with none\n",
        "\n",
        "df = pd.DataFrame({'SentenceTypeAllChargesAtConvictionInContactEvent','ImposedSentenceAllChargeInContactEvent'})\n",
        "slay = pt['SentenceTypeAllChargesAtConvictionInContactEvent'] + ',' + pt['ImposedSentenceAllChargeInContactEvent']\n",
        "print(slay) #create new variable combining the two variables' values, but separating with a comma so it reads as sentence type, followed by amount of years per that sentence type (if any)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GVyjLUIU2Cw",
        "outputId": "ff2cfa77-fb25-403d-ecba-5bad3ff008c9"
      },
      "id": "_GVyjLUIU2Cw",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-2b4d636f6cde>:11: DtypeWarning: Columns (10,17,18,19,20,21,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  sharky = pd.read_csv('./Assignment2/data/sharks.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9",
      "metadata": {
        "id": "c11bcd96-2834-41a4-80fe-d354b4277fd9"
      },
      "source": [
        "**Q3.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "The \"relevant\" variables for this question are:\n",
        "  - `level` - Level of institution (4-year, 2-year)\n",
        "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
        "  - `control` - Public, Private not-for-profit, Private for-profit\n",
        "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
        "\n",
        "1. Load the `./data/college_completion.csv` data with Pandas.\n",
        "2. What are are the dimensions of the data? How many observations are there? What are the variables included? Use `.head()` to examine the first few rows of data.\n",
        "3. Cross tabulate `control` and `level`. Describe the patterns you see.\n",
        "4. For `grad_100_value`, create a histogram, kernel density plot, boxplot, and statistical description.\n",
        "5. For `grad_100_value`, create a grouped kernel density plot by `control` and by `level`. Describe what you see. Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `grad_100_value` by `level` and `control`. Which institutions appear to have the best graduation rates?\n",
        "6. Create a new variable, `df['levelXcontrol']=df['level']+', '+df['control']` that interacts level and control. Make a grouped kernel density plot. Which institutions appear to have the best graduation rates?\n",
        "7. Make a kernel density plot of `aid_value`. Notice that your graph is \"bi-modal\", having two little peaks that represent locally most common values. Now group your graph by `level` and `control`. What explains the bi-modal nature of the graph? Use `groupby` and `.describe` to make grouped calculations of statistical descriptions of `aid_value` by `level` and `control`.\n",
        "8. Make a scatterplot of `grad_100_value` by `aid_value`. Describe what you see. Now make the same plot, grouping by `level` and then `control`. Describe what you see. For which kinds of institutions does aid seem to increase graduation rates?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Load Dataset\n",
        "import pandas as pd\n",
        "cc = pd.read_csv('./Assignment2/data/college_completion.csv')\n",
        "\n",
        "#2\n",
        "cc.head()\n",
        "  #When using the head() function, there are 5 rows x 63 columns- columns denoting the amount of variables.The variables are all related to something that has to do with college (name of college, city/state, type of school, transfer rate, etc.)\n",
        "\n",
        "#3 Cross Tabulate\n",
        "pd.crosstab(cc['control'], cc['level'])\n",
        "  # This shows that overall, most graduate private college from a 4-year program over a 2-year program(excluding for or not-for-profit). More specifically, there is a greater preference for this at a private not-for-profit 4-year program. Additionally, most graduate from a 2-year public college over a 4-year program.\n",
        "\n",
        "#4 grad_100_value\n",
        "  #Histogram\n",
        "cc['grad_100_value'] = cc['grad_100_value']\n",
        "var = 'grad_100_value'\n",
        "cc[var].hist(), '\\n'\n",
        "  #Kernel Density Plot\n",
        "cc['grad_100_value'] = cc['grad_100_value']\n",
        "ker = 'grad_100_value'\n",
        "cc[ker].plot.density()\n",
        "  #Boxplot\n",
        "cc['grad_100_value'] = cc['grad_100_value']\n",
        "bp = 'grad_100_value'\n",
        "cc.boxplot(column = 'grad_100_value')\n",
        "  #Statistical Description\n",
        "cc['grad_100_value'] = cc['grad_100_value']\n",
        "descr = cc['grad_100_value'].describe()\n",
        "print(descr,'\\n')\n",
        "\n",
        "#5 group grad_100_value\n",
        "  #Grouped kernel Density Plot control and level\n",
        "group_by = ['control','level']\n",
        "var = 'grad_100_value'\n",
        "df_wide = cc.pivot(columns=group_by,values=var)\n",
        "df_wide.plot.density() #2-year public institutions have the highest graduation rate\n",
        "  #Describe\n",
        "var = 'grad_100_value'\n",
        "cc[var].describe() #2-year public institutions have the highest graduation rate\n",
        "\n",
        "#6 New Variable\n",
        "cc['levelXcontrol']=cc['level']+', '+cc['control']\n",
        "var = 'grad_100_value'\n",
        "cc[var] = pd.to_numeric(cc[var], errors='coerce')\n",
        "df_wide = cc.pivot(columns=cc['levelXcontrol'],values=var)\n",
        "df_wide.plot.density()\n",
        "\n",
        "#7 Binodal Kernel Density Plot\n",
        "cc['aid_value'] = cc['aid_value']\n",
        "ker = 'aid_value'\n",
        "cc[ker].plot.density() #the binodal nature of this graph shows that there are two primary amounts of financial aid which students recieve across all types of collegiate institutions\n",
        "  #groupby\n",
        "cc['aid_value'] = cc['aid_value']\n",
        "ker = 'aid_value'\n",
        "group_by = ['control','level']\n",
        "df_wide = cc.pivot(columns=group_by,values=ker)\n",
        "df_wide.plot.density()\n",
        "df_wide.describe()\n",
        "\n",
        "#8 Scatterplot\n",
        "cc.plot.scatter(y='grad_100_value',x='aid_value') #there is a higher concentration of financial aid which falls in the $0 - $10,000 range and a graduation rate from 0-30%, implying that the lower the aid, the lower the graduation rate.\n",
        "  #groupby level\n",
        "grouped = cc.groupby('level')\n",
        "grouped.plot(kind='scatter', x='aid_value', y='grad_100_value')\n",
        "    #4-year institutions gives most aid to show a higher graduation rate\n",
        "  #groupby control\n",
        "grouped = cc.groupby('control')\n",
        "grouped.plot(kind='scatter', x='aid_value', y='grad_100_value')\n",
        "    #private not-for-profit gives most aid to show a higher graduation rate"
      ],
      "metadata": {
        "id": "PauHuTBHC5XD"
      },
      "id": "PauHuTBHC5XD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}